{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Showcase with Shakespeare in German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for colab only\n",
    "#for problems with package versions look in the requirements-colab-freeze.txt and requirements-pip-freeze.txt files\n",
    "!pip install tiktoken\n",
    "!git clone https://github.com/phonosync/demo_llm\n",
    "!mv GenAI_LLM/* ./\n",
    "import gdown\n",
    "file_id = '19caXJPPRXEHm18Y5L5YI0erfCSaYbO4T'\n",
    "url = f'https://drive.google.com/uc?id={file_id}'\n",
    "output = 'out-shakespeare-deutsch/ckpt_pretrained.pt'\n",
    "\n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 29.94M\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "from model import GPTConfig, GPT\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "out_dir = 'out-shakespeare-deutsch'\n",
    "num_samples = 3 # number of samples to draw\n",
    "max_new_tokens = 200 # number of tokens generated in each sample\n",
    "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "seed = 1337\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "device = 'cpu'\n",
    "dtype = 'float16'\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext()\n",
    "\n",
    "# model\n",
    "ckpt_path = os.path.join(out_dir, 'ckpt_pretrained.pt')\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "decode = lambda l: enc.decode(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Dieser Satz wird als Anfang genutzt erscheinen,\n",
      "Und glaubt, daß der Tod ein Feind\n",
      "Und seiner Ehre stets und das schlimmste\n",
      "Kann ich doch und weiß, wie man ihn vergessen.\n",
      "\n",
      "ROSSE\n",
      "Die Hände, die Wohl verdient,\n",
      "So waren sie, den Tag vor dem Tod,\n",
      "Und widersprach das Tod des Anblick.\n",
      "Die Sicherheit stolzer wahren, wird von ihm\n",
      "Beim Himmel, in Vermätern, wo er sich\n",
      "Den Vorsatz des Dunsinan dem unserm Grames Haupt,\n",
      "Wo sein so in der Natur, mit ihm verschworen,\n",
      "Wo er verhobt, daß er stirbt.\n",
      "\n",
      "MACBETH\n",
      "Gift er so,\n",
      "---------------\n",
      "Dieser Satz wird als Anfang genutzt erschlagen.\n",
      "\n",
      "MALCOLM\n",
      "Noch soll er nicht, wenn man ihn dran,\n",
      "Denn unverzückt, als er die Sünden\n",
      "Auf unsre Sache, daß unser Schicksal\n",
      "Gewinn.--Haben auch den Staat!\n",
      "\n",
      "MACBETH\n",
      "Wäre darüber zu spüten.\n",
      "\n",
      "LADY MACDUFF\n",
      "Ich will dich nicht stehn,\n",
      "Daß es erscheint; doch wie er kann,\n",
      "Worin, und wie sie im Nacht verriet.\n",
      "\n",
      "MACBETH\n",
      "Es tut gut, wenn er ihm zu sagen.\n",
      "\n",
      "MACBETH\n",
      "Ich haß es tun, daß man ihm folgen.\n",
      "\n",
      "MACBETH\n",
      "Jetzt, wie er auf\n",
      "---------------\n",
      "Dieser Satz wird als Anfang genutzt erten;\n",
      "Der seine schlimmste,\n",
      "All ein lebendes an sie\n",
      "Entfaulen werden.  Das ist erschlug,\n",
      "So hohe Bett, er wie ein schönrer Geist\n",
      "Der, wie ein Zeuge einer Stirn\n",
      "Und wie der Träumen, seinen Tod\n",
      "Zu schaffen, wenn er führt, wie ein Weib,\n",
      "Da er ein Trauer hemmt er ihm angebild.\n",
      "\n",
      "ERSTER MÖRDER\n",
      "Mich würd ein vor mir zu dessen,\n",
      "Und jeder Julia aus dem gefießen.\n",
      "\n",
      "([Drei Hexen.])\n",
      "\n",
      "MACBETH\n",
      "O\n",
      "Wer weiß?\n",
      "\n",
      "([Alle ab.)\n",
      "\n",
      "MACBETH\n",
      "Ja\n"
     ]
    }
   ],
   "source": [
    "start = \"Dieser Satz wird als Anfang genutzt \" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "\n",
    "start_ids = encode(start)\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(num_samples):\n",
    "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "            print('---------------')\n",
    "            print(decode(y[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
