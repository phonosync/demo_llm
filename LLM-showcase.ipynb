{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7H8BV4kLLtd"
      },
      "source": [
        "# LLM Showcase with Shakespeare in German"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmBusQioLLtf"
      },
      "outputs": [],
      "source": [
        "#for colab only\n",
        "#for problems with package versions look in the requirements-colab-freeze.txt and requirements-pip-freeze.txt files\n",
        "!pip install tiktoken\n",
        "!git clone https://github.com/phonosync/demo_llm\n",
        "!mv demo_llm/* ./\n",
        "import gdown\n",
        "file_id = '19caXJPPRXEHm18Y5L5YI0erfCSaYbO4T'\n",
        "url = 'https://drive.switch.ch/index.php/s/cPbmwNdKjZMUN9G/download' # f'https://drive.google.com/uc?id={file_id}'\n",
        "output = 'out-shakespeare-deutsch/ckpt_pretrained.pt'\n",
        "\n",
        "gdown.download(url, output, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqbVAvgILLtg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tiktoken\n",
        "from contextlib import nullcontext\n",
        "import torch\n",
        "from model import GPTConfig, GPT\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "out_dir = 'out-shakespeare-deutsch'\n",
        "num_samples = 3 # number of samples to draw\n",
        "max_new_tokens = 200 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "seed = 1337\n",
        "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "device = 'cpu'\n",
        "dtype = 'float16'\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext()\n",
        "\n",
        "# model\n",
        "ckpt_path = os.path.join(out_dir, 'ckpt_pretrained.pt')\n",
        "checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "gptconf = GPTConfig(**checkpoint['model_args'])\n",
        "model = GPT(gptconf)\n",
        "state_dict = checkpoint['model']\n",
        "unwanted_prefix = '_orig_mod.'\n",
        "for k,v in list(state_dict.items()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
        "decode = lambda l: enc.decode(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AS9v7ZBaLLth"
      },
      "outputs": [],
      "source": [
        "start = \"Dieser Satz wird als Anfang genutzt \" # oder \"<|endoftext|>\" oder etc. kann auch als File bereitgestellt werden: \"FILE:prompt.txt\"\n",
        "\n",
        "start_ids = encode(start)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print('---------------')\n",
        "            print(decode(y[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9NMo1Ma5N4rl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}